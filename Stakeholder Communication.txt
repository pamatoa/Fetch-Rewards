Stakeholder Communication

Questions about the Data:
1.Data Source Specifics: While I know the data comes from JSON files (receipts.json, brands.json, users.json), I need more specifics. Are these files generated by a single system or multiple systems? Are they exports from a database, API responses, or user-generated content? Understanding the origin will help trace data quality issues.
2.Data Collection Process: How is the data collected and populated into these JSON files? Is it automated, manual, or a combination? Knowing the process will help identify potential points of failure.
3.Data Definitions & Constraints: Are there formal definitions or constraints for each field? For example, what are the valid values for payment_method, brand_category, or rewardsReceiptStatus? Are there any length or format restrictions for text fields? Clear definitions are crucial for validation.
4.Data Update Frequency: How often is this data updated? Is it a one-time export, a daily batch, or a near real-time stream? This will impact how we plan for data refresh and analysis.
5.Data Volume: What is the expected volume of data? How many records are we expecting in each JSON file? This will help in planning for performance and scalability.
6.rewardsReceiptStatus Values: What are all the possible values for rewardsReceiptStatus? I've seen 'Accepted' and 'Rejected', but are there others?
7.Relationship Cardinality: Is there a strict one-to-many relationship between users and receipts, and brands and receipts? Or are there possibilities of many-to-many relationships?
8.Data Lineage: Is there any information about the data lineage? How does the data flow from the source to these JSON files?

How I Discovered Data Quality Issues:
I discovered data quality issues by using SQL queries (as shown in data_quality_checks.sql) against the relational model I created. These queries looked for:
	• NULL Values: Missing values in key columns like user_id, brand_id, purchase_date, etc.
	• Duplicate Records: Duplicate entries based on primary key columns in Users, Brands, and Receipts.
	• Invalid Date Formats: Dates that don't conform to the ISO 8601 format.
	• Inconsistent Categorical Data: Inconsistent values in payment_method and brand_category.
	• Invalid Numeric Values: Negative or zero values in total_amount, quantity, and price.
	• Orphaned Records: Receipts without corresponding users or brands.
	• Invalid Email Formats: Emails that don't conform to a basic email format.

What I Need to Know to Resolve the Data Quality Issues:
1.Data Validation Rules: I need clear and specific validation rules for each field. This includes data types, formats, allowed values, and constraints.
2.Data Correction Strategy: How should I handle invalid data? Should I discard it, impute it, or correct it based on other data sources? I need a clear strategy for data cleansing.
3.Data Source Contacts: Who are the contacts for the systems that generate these JSON files? This will help in understanding the data collection process and resolving issues at the source.
4.Business Impact: What is the business impact of each data quality issue? This will help in prioritizing the issues to be resolved.
5.Data Governance Policies: Are there any data governance policies that I need to follow? This will help in ensuring that I am handling the data in a compliant manner.
6.Error Handling: How should I handle errors during data loading and processing? Should I log them, alert someone, or retry the process?

Other Information Needed to Optimize Data Assets:
1.Business Use Cases: What are the specific business use cases for this data? Understanding the use cases will help in optimizing the data model and queries.
2.Reporting Requirements: What are the reporting requirements? This will help in designing the data model for efficient reporting.
3.Data Retention Policies: What are the data retention policies? This will help in planning for data storage and archival.
4.Data Security Requirements: What are the data security requirements? This will help in ensuring that the data is protected from unauthorized access.
5.Data Integration Needs: How will this data be integrated with other data sources? This will help in planning for data integration and interoperability.
6.Performance Requirements: What are the performance requirements for data access and analysis? This will help in optimizing the data model and queries for performance.

Performance and Scaling Concerns and Plans:
1. Data Volume: As the data volume grows, query performance may degrade. To address this, I plan to:
	o Indexing: Add indexes to key columns to improve query performance.
	o Partitioning: Partition the data based on date or other criteria to improve query performance.
	o Materialized Views: Create materialized views for frequently used queries to improve performance.
2. Data Update Frequency: As the data update frequency increases, the system may become overloaded. To address this, I plan to:
	o Batch Processing: Use batch processing to update the data in chunks.
	o Incremental Updates: Implement incremental updates to only update the changed data.
	o Caching: Use caching to reduce the load on the database.
3. Concurrency: As the number of users accessing the data increases, the system may become overloaded. To address this, I plan to:
	o Connection Pooling: Use connection pooling to manage database connections efficiently.
	o Load Balancing: Use load balancing to distribute the load across multiple servers.
	o Query Optimization: Optimize queries to reduce the load on the database.
4. Scalability: As the data volume and user base grow, the system may need to scale. To address this, I plan to:
	o Horizontal Scaling: Scale the database horizontally by adding more servers.
	o Cloud Services: Use cloud services to scale the system on demand.
	o Database Optimization: Optimize the database schema and queries for scalability.
5. Data Transformation: Complex data transformations can impact performance. To address this, I plan to:
	o Optimize Transformations: Optimize data transformation logic for performance.
	o Use Data Pipelines: Use data pipelines to manage data transformations efficiently.
	o Parallel Processing: Use parallel processing to speed up data transformations.
